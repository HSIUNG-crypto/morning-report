import json
from datetime import datetime, timezone
from zoneinfo import ZoneInfo
import os
import time

import requests
import feedparser
import yfinance as yf

TIMEOUT = 12
DATA_FILE = os.path.join(os.path.dirname(__file__), "data.json")
MP3_FILE = "morning.mp3"

def safe_get_json(url, label="", params=None, headers=None):
    try:
        r = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)
        r.raise_for_status()
        try:
            return r.json()
        except Exception:
            print(f"âš ï¸ {label} å›å‚³é JSONï¼Œç•¥éã€‚")
            return {}
    except requests.RequestException as e:
        print(f"âš ï¸ {label} ç¶²è·¯/æœå‹™éŒ¯èª¤ï¼š{e}")
        return {}

# ---------- åŒ¯ç‡ ----------
def get_exchange_rates(prev=None):
    data = safe_get_json("https://open.er-api.com/v6/latest/USD", label="åŒ¯ç‡")
    rates = data.get("rates", {})

    # ğŸ§© æ–°å¢é€™æ®µé˜²å‘†ï¼šå¦‚æœ rates ç‚ºç©ºï¼Œä½¿ç”¨ä¸Šä¸€ç‰ˆè³‡æ–™
    if not rates:
        print("âš ï¸ åŒ¯ç‡ API ç„¡æ³•å›å‚³ ratesï¼Œä½¿ç”¨ä¸Šä¸€ç‰ˆè³‡æ–™ã€‚")
        return prev.get("exchange_rates", {}) if prev else {}

    wanted = ["TWD","JPY","EUR","GBP","CNY","AUD","CAD","CHF","HKD","KRW","SGD","INR"]
    out = {}
    for code in wanted:
        if code in rates:
            out[f"USD/{code}"] = round(float(rates[code]), 4)
    return out

# ---------- æŒ‡æ•¸ï¼ˆå«é‡è©¦ã€å‚™æ´ã€æ²¿ç”¨å‰å€¼ï¼‰ ----------
IDX_TICKERS = {
    "^DJI":"é“ç“Š", "^IXIC":"é‚£æ–¯é”å…‹", "^GSPC":"S&P 500", "^N225":"æ—¥ç¶“225",
    "^GDAXI":"å¾·åœ‹DAX", "^FTSE":"è‹±åœ‹FTSE", "^HSI":"æ†ç”ŸæŒ‡æ•¸", "^TWII":"å°ç£åŠ æ¬Š"
}
def get_stock_indexes(prev=None):
    result = {}
    tickers = " ".join(IDX_TICKERS.keys())
    ok = False
    for attempt in range(3):
        try:
            data = yf.download(tickers, period="6d", interval="1d",
                               progress=False, group_by='ticker', threads=False)
            ok = True
            break
        except Exception as e:
            print(f"âš ï¸ yfinance å¤±æ•—({attempt+1}/3)ï¼š{e}")
            time.sleep(2)
    if ok:
        try:
            # yfinance æœ‰å…©ç¨®çµæ§‹ï¼Œçµ±ä¸€è™•ç†
            for t, name in IDX_TICKERS.items():
                try:
                    close_series = data[t]["Close"] if isinstance(data, dict) and t in data else data["Close"][t]
                except Exception:
                    close_series = data["Close"][t]
                price = float(close_series.tail(1).values[0])
                if len(close_series) >= 2:
                    prev_close = float(close_series.tail(2).values[0])
                    chg = ((price - prev_close) / prev_close) * 100 if prev_close else 0.0
                else:
                    chg = 0.0
                result[name] = {"price": round(price,2), "change": round(chg,2)}
        except Exception as e:
            print(f"âš ï¸ yfinance è§£æå¤±æ•—ï¼š{e}")
    # æŠ“ä¸åˆ°å°±æ²¿ç”¨å‰å€¼ï¼ˆé¿å…å‰ç«¯ç©ºç™½ï¼‰
    if not result and prev:
        print("â„¹ï¸ æŒ‡æ•¸æŠ“å–å¤±æ•—ï¼Œæ²¿ç”¨ä¸Šä¸€ç‰ˆæ•¸æ“šã€‚")
        return prev
    return result

# ---------- æ–°èï¼ˆåˆ†ä¸‰é¡ï¼‰ ----------
RSS_ECONOMY = [
    "http://feeds.bbci.co.uk/news/business/rss.xml",
    "https://feeds.reuters.com/reuters/businessNews",
    "https://www.ft.com/rss/home/asia",  # FT å…¨ç«™ RSSï¼Œä»èƒ½æŒ‘åˆ°å•†æ¥­ç¶“æ¿Ÿ
]
RSS_MARKETS = [
    "https://feeds.reuters.com/reuters/marketsNews",
    "https://www.marketwatch.com/feeds/topstories",  # MarketWatch
    "https://feeds.a.dj.com/rss/RSSMarketsMain.xml",  # Bloomberg éƒ¨åˆ† feedï¼ˆç¤ºæ„ï¼‰
]
RSS_AI = [
    "https://feeds.arstechnica.com/arstechnica/technology-lab", # ç§‘æŠ€/AI
    "https://www.theverge.com/rss/index.xml",
    "https://feeds.feedburner.com/TechCrunch/"
]

# ğŸ§© RSS è®€å–ï¼šåŠ ä¸Š User-Agentï¼Œé¿å…ä¼ºæœå™¨æ‹’çµ•åŒ¿åé€£ç·š
HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; NightCatMorning/1.0; +https://github.com/HSIUNG-crypto/morning-report)"
}

def fetch_rss_batch(urls, max_items=5):
    items = []
    for url in urls:
        try:
            # âœ… é—œéµä¿®æ”¹é»ï¼šåŠ ä¸Š request_headers=HEADERS
            d = feedparser.parse(url, request_headers=HEADERS)
            for e in d.entries[:max_items]:
                title = e.get("title","").strip()
                link = e.get("link","").strip()
                if title and link:
                    items.append({"title": title, "url": link})
        except Exception as ex:
            print(f"âš ï¸ RSS è®€å–å¤±æ•—ï¼š{url} -> {ex}")
    # å»é‡ + å–å‰ N
    seen, uniq = set(), []
    for it in items:
        if it["title"] not in seen:
            seen.add(it["title"])
            uniq.append(it)
    return uniq[:max_items]

def short_forecast_from_titles(titles, domain):
    """
    è¶…ç°¡çŸ­ 100 å­—å…§ã€Œæƒ…å‹¢é æ¸¬ã€ï¼šä»¥é—œéµè©æ–¹å‘+ä¿å®ˆç”¨èªï¼Œé¿å…éåº¦æ­¦æ–·ã€‚
    domain: 'economy' | 'markets' | 'ai'
    """
    text = "ã€".join(titles[:5])
    text = text.lower()
    pos = sum(text.count(k) for k in ["growth","expansion","optimism","rally","recover","boom","record","surge"])
    neg = sum(text.count(k) for k in ["slow","recession","decline","fall","slump","risk","crisis","cut"])
    neu = sum(text.count(k) for k in ["flat","mixed","steady","unchanged","pause"])
    sentiment = "åç©©" if max(pos,neg,neu)==neu else ("åå¼·" if pos>=neg else "åå¼±")

    if domain=="economy":
        base = f"æ•´é«”ç¶“æ¿Ÿè¨Šè™Ÿ{sentiment}ã€‚"
        tip = "ç•™æ„é€šè†¨èˆ‡æ”¿ç­–è·¯å¾‘ï¼Œæ§åˆ¶éƒ¨ä½ï¼Œé‡æ³¢å‹•ä»¥åˆ†æ‰¹ç‚ºå®œã€‚"
    elif domain=="markets":
        base = f"å¸‚å ´æƒ…ç·’{sentiment}ï¼Œæ¿å¡Šè¼ªå‹•å¯èƒ½åŠ é€Ÿã€‚"
        tip = "å»ºè­°èšç„¦é«˜æµå‹•æ€§è³‡ç”¢ï¼Œåš´è¨­åœæèˆ‡é¢¨éšªé™é¡ã€‚"
    else: # ai
        base = f"AI å‹•èƒ½{sentiment}ï¼ŒæŠ€è¡“èˆ‡ç›£ç®¡æ¶ˆæ¯äº¤éŒ¯ã€‚"
        tip = "çŸ­æœŸä»¥å¤§å‹é›²/æ™¶ç‰‡ç‚ºä¸»è»¸ï¼Œç•™æ„è©•åƒ¹å£“åŠ›ã€‚"
    s = f"{base}{tip}"
    return s[:95] + "â€¦" if len(s)>100 else s

def load_prev():
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}
    return {}

def calc_changes(curr: dict, prev: dict):
    out = {}
    for k,v in curr.items():
        p = prev.get(k)
        if isinstance(v,(int,float)) and isinstance(p,(int,float)) and p:
            out[k] = round((v - p)/p*100, 2)
        else:
            out[k] = None
    return out

def build_summary(ex, exch, st, news_ec, news_mk, news_ai):
    parts = []
    if ex:
        fx_line = []
        for k in ["USD/TWD","USD/JPY","USD/EUR"]:
            if k in ex:
                delta = exch.get(k)
                fx_line.append(f"{k.replace('USD/','')} {ex[k]}({'' if delta is None else ('+' if delta>0 else '')}{'' if delta is None else delta}%)")
        if fx_line:
            parts.append("åŒ¯ç‡ï¼š" + "ï¼Œ".join(fx_line))
    if st:
        best = sorted(st.items(), key=lambda kv: kv[1].get("change",0), reverse=True)
        top = best[0] if best else None
        if top:
            parts.append(f"è‚¡å¸‚ï¼š{top[0]} è®Šå‹• {top[1]['change']}%ã€‚")
    if news_ec:
        parts.append("ç¶“æ¿Ÿï¼š" + news_ec[0]["title"])
    return "ã€‚".join(parts) if parts else "ä»Šæ—¥é‡é»å·²æ›´æ–°ï¼Œè«‹æŸ¥çœ‹æ¿å¡Šã€‚"

def maybe_make_tts(summary_text:str):
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("â„¹ï¸ æœªè¨­å®š OPENAI_API_KEYï¼Œè·³é MP3 ç”¢ç”Ÿã€‚")
        return
    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key)
        speech = client.audio.speech.create(
            model="gpt-4o-mini-tts",
            voice="alloy",
            input=summary_text
        )
        with open(MP3_FILE,"wb") as f:
            f.write(speech.read())
        print("ğŸ§ å·²ç”¢ç”Ÿæ™¨è®€ MP3ã€‚")
    except Exception as e:
        print(f"âš ï¸ MP3 ç”¢ç”Ÿå¤±æ•—ï¼š{e}ï¼ˆå°‡ä½¿ç”¨ç€è¦½å™¨æœ—è®€ fallbackï¼‰")

def main():
    prev = load_prev()

    ex = get_exchange_rates(prev=prev)
    exch = calc_changes(ex, prev.get("exchange_rates", {}))

    st = get_stock_indexes(prev=prev.get("stocks"))
    news_economy = fetch_rss_batch(RSS_ECONOMY, 5)
    news_markets = fetch_rss_batch(RSS_MARKETS, 5)
    news_ai = fetch_rss_batch(RSS_AI, 5)

    forecast_economy = short_forecast_from_titles([n["title"] for n in news_economy], "economy") if news_economy else ""
    forecast_markets = short_forecast_from_titles([n["title"] for n in news_markets], "markets") if news_markets else ""
    forecast_ai = short_forecast_from_titles([n["title"] for n in news_ai], "ai") if news_ai else ""

    summary_text = build_summary(ex, exch, st, news_economy, news_markets, news_ai)

    now_tpe = datetime.now(ZoneInfo("Asia/Taipei"))
    now_utc = datetime.now(timezone.utc)

    result = {
        "updated_at": now_tpe.strftime("%Y-%m-%d %H:%M:%S") + " (å°åŒ—æ™‚é–“)",
        "updated_at_utc": now_utc.strftime("%Y-%m-%d %H:%M:%S") + " (UTC)",
        "exchange_rates": ex,
        "exchange_changes": exch,
        "stocks": st,
        "news_economy": news_economy,
        "news_markets": news_markets,
        "news_ai": news_ai,
        "forecast_economy": forecast_economy,
        "forecast_markets": forecast_markets,
        "forecast_ai": forecast_ai,
        "summary_text": summary_text
    }
    with open(DATA_FILE,"w",encoding="utf-8") as f:
        json.dump(result,f,ensure_ascii=False,indent=2)
    print("âœ… ä»Šæ—¥æ—©å ±è³‡æ–™å·²æ›´æ–°å®Œæˆï¼")
    maybe_make_tts(summary_text)

if __name__ == "__main__":
    main()
